{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to generate results from planted audio detection part of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planted audio detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from sklearn import svm\n",
    "\n",
    "import cv2\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from stingray.lightcurve import Lightcurve\n",
    "from stingray.bispectrum import Bispectrum\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters used in the research\n",
    "SEGMENT_SIZE = 400\n",
    "SEGMENT_OVERLAP = 200\n",
    "\n",
    "min_slice_size = 6400\n",
    "max_slice_size = 9600\n",
    "SLICE_SIZE = 3200\n",
    "SLICE_OVERLAP = 1600\n",
    "\n",
    "random_controller = 0\n",
    "device = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model to use\n",
    "class ResNetMulti(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetMulti, self).__init__()\n",
    "        self.model = models.resnet50(pretrained=False, num_classes=1)\n",
    "        self.model.conv1 = nn.Conv2d(5, 64, kernel_size=(5, 5), stride=(2, 2), padding=(3, 3), bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return nn.Sigmoid()(self.model(x))\n",
    "\n",
    "# merging 2 audios' random slices\n",
    "def merge_2_audio(real, fake):\n",
    "    global random_controller\n",
    "    random.seed(random_controller)\n",
    "    \n",
    "    parts = []\n",
    "    labels = []\n",
    "    index = 0\n",
    "    \n",
    "    while len(real) + len(fake) > 0:\n",
    "        length = random.randint(min_slice_size, max_slice_size)\n",
    "        if random.random() < 0.5 and len(real) > 0:\n",
    "            min_len = min(length, len(real))\n",
    "            parts.append(real[:min_len])\n",
    "            labels.append(0)\n",
    "            real = real[min_len:]\n",
    "            index += min_len\n",
    "        else:\n",
    "            min_len = min(length, len(fake))\n",
    "            parts.append(fake[:min_len])\n",
    "            fake = fake[min_len:]\n",
    "            index += min_len\n",
    "            labels.append(1)\n",
    "    return parts, labels\n",
    "\n",
    "# check the create_image_dataset.py for comments\n",
    "def get_features(data, samplerate, max_K=-1):\n",
    "    num_segments = (len(data) - SEGMENT_SIZE) // SEGMENT_OVERLAP + 1\n",
    "    if max_K > 0:\n",
    "        num_segments = min(num_segments, max_K)\n",
    "\n",
    "    RC_layers = np.zeros((num_segments, SEGMENT_SIZE+1, SEGMENT_SIZE+1), dtype=complex)\n",
    "    cum3_sum = np.zeros((SEGMENT_SIZE+1, SEGMENT_SIZE+1))\n",
    "\n",
    "    time_values = np.linspace(0, SEGMENT_SIZE / samplerate, SEGMENT_SIZE)\n",
    "    for idx, segment_start in enumerate(range(0, len(data), SEGMENT_OVERLAP)):\n",
    "        if idx == num_segments:\n",
    "            break\n",
    "        segment = data[segment_start:segment_start + SEGMENT_SIZE]\n",
    "        \n",
    "        lc = Lightcurve(time_values, segment)\n",
    "        bs = Bispectrum(lc, window=\"hamming\")\n",
    "\n",
    "        mag, phase, cum3 = bs.bispec_mag, bs.bispec_phase, bs.cum3\n",
    "        cum3_sum = cum3_sum + cum3\n",
    "\n",
    "        R = mag * np.cos(phase)\n",
    "        C = mag * np.sin(phase)\n",
    "        RC_layers[idx] = R + C * 1j\n",
    "\n",
    "    return RC_layers, cum3_sum/num_segments\n",
    "\n",
    "# check the create_image_dataset.py for comments\n",
    "def create_signature_image(RC_layers):\n",
    "    RC_layers = RC_layers[..., np.newaxis]\n",
    "    signature_image = np.zeros(RC_layers.shape[1:], dtype=complex)\n",
    "    tops = np.sum(RC_layers, axis=0)\n",
    "\n",
    "    signature_image = np.reshape(np.array([tops[r][c]/(np.sqrt(np.dot(RC_layers[:,r,c,:].T,np.conjugate(RC_layers[:,r,c,:])).real) + 0.0001) \n",
    "                                        for r in range(signature_image.shape[0]) \n",
    "                                        for c in range(signature_image.shape[1])]), signature_image.shape)\n",
    "\n",
    "    # list comprehension is for this for loop\n",
    "    # for r in range(signature_image.shape[0]):\n",
    "    #     for c in range(signature_image.shape[1]):\n",
    "    #         L = RC_layers[:,r,c,:]\n",
    "    #         top = tops[r][c]\n",
    "    #         bottom = np.sqrt(np.dot(L.T, np.conjugate(L)).real)\n",
    "    #         signature_image[r,c] = top/(bottom + 0.0001)\n",
    "\n",
    "    return signature_image\n",
    "\n",
    "# check the create_image_dataset.py for comments\n",
    "def audio_to_images(data, samplerate):\n",
    "    RC_layers, cum3_avg = get_features(data, samplerate, max_K=-1)\n",
    "    signature_image = create_signature_image(RC_layers)\n",
    "\n",
    "    absolute = np.absolute(signature_image)\n",
    "    absolute_norm = (absolute - absolute.min()) / (absolute.max() - absolute.min())\n",
    "\n",
    "    angle = np.angle(signature_image)\n",
    "    angle_norm = (angle - angle.min()) / (angle.max() - angle.min())\n",
    "\n",
    "    real = signature_image.real\n",
    "    real_norm = (real - real.min()) / (real.max() - real.min())\n",
    "\n",
    "    imag = signature_image.imag\n",
    "    imag_norm = (imag - imag.min()) / (imag.max() - imag.min())\n",
    "\n",
    "    cum3_norm = (cum3_avg - cum3_avg.min()) / (cum3_avg.max() - cum3_avg.min())\n",
    "\n",
    "    return absolute_norm[:,:,0], angle_norm[:,:,0], real_norm[:,:,0], imag_norm[:,:,0], cum3_norm\n",
    "\n",
    "# check the create_image_dataset.py for comments\n",
    "def get_slice_features(audio):\n",
    "    slice_features = []\n",
    "    for slice_start in tqdm(range(0, len(audio), SLICE_OVERLAP)):\n",
    "        slice = audio[slice_start:slice_start + SLICE_SIZE]\n",
    "        absolute_norm, angle_norm, real_norm, imag_norm, cum3_norm = audio_to_images(slice, 16000)\n",
    "        stacked = np.stack([absolute_norm, angle_norm, cum3_norm, imag_norm, real_norm])[np.newaxis, ...]\n",
    "        slice_features.append(stacked)\n",
    "\n",
    "    return slice_features\n",
    "\n",
    "# passes features to the model\n",
    "def pass_features_to_model(model, slice_features, batch_size=4):\n",
    "    features_batch = np.vstack(slice_features)\n",
    "    features_batch = features_batch.astype(np.float32)\n",
    "\n",
    "    input_tensor = torch.from_numpy(features_batch.astype(np.float32)).to(device)\n",
    "\n",
    "    dataset = TensorDataset(input_tensor)\n",
    "    batch_size = 4  # Adjust batch size as needed\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    probs_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_tensor = batch[0].to(device)\n",
    "            probs = model(input_tensor)\n",
    "            probs_list.append(probs.cpu().numpy())\n",
    "\n",
    "    # Concatenate all batches into a single numpy array\n",
    "    probs_array = np.concatenate(probs_list, axis=0)\n",
    "\n",
    "    return probs_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates each slices' probability predicted by the model\n",
    "def get_slice_probs(real_audio_path, fake_audio_path):\n",
    "    global device\n",
    "    real, _ = librosa.load(real_audio_path, sr=16000)\n",
    "    fake, _ = librosa.load(fake_audio_path, sr=16000)\n",
    "\n",
    "    parts, labels = merge_2_audio(real, fake)\n",
    "\n",
    "    audio = np.hstack(parts)\n",
    "    slice_features = get_slice_features(audio)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = ResNetMulti().to(device)\n",
    "    model.load_state_dict(torch.load(\"\"))\n",
    "    model = model.eval()\n",
    "\n",
    "    probs = pass_features_to_model(model, slice_features, 4)\n",
    "\n",
    "    return audio, parts, labels, probs\n",
    "\n",
    "# draws plot of predictions\n",
    "def draw_plot(audio, parts, labels, probs_list, SLICE_SIZE_AND_OVERLAPS, title):\n",
    "    num_plots = len(probs_list)\n",
    "    fig, axs = plt.subplots(num_plots, 1, figsize=(14, 5 * num_plots), sharex=True)\n",
    "\n",
    "    for i, (probs, (SLICE_SIZE, SLICE_OVERLAP)) in enumerate(zip(probs_list, SLICE_SIZE_AND_OVERLAPS)):\n",
    "        ax = axs[i] if num_plots > 1 else axs  # Handle single subplot case\n",
    "        start_index = 0\n",
    "        for m, l in zip(parts, labels):\n",
    "            end_index = start_index + len(m)\n",
    "            c = \"red\" if l == 1 else \"blue\"\n",
    "            ax.plot(list(range(start_index, end_index)), m, color=c)\n",
    "            start_index = end_index\n",
    "\n",
    "        # confidence limits\n",
    "        ax.axhline(y=0.5, color='black', linestyle=':')\n",
    "        ax.axhline(y=-0.5, color='black', linestyle=':')\n",
    "\n",
    "        # confidence\n",
    "        detection_points = range(SLICE_SIZE//2, len(audio) + SLICE_OVERLAP, SLICE_OVERLAP)\n",
    "        zero_line = [0 for _ in probs]\n",
    "        prob_points = probs[:, 0] - 0.5\n",
    "        ax.plot(detection_points, prob_points, color=\"black\", zorder=100, linestyle='--', alpha=0.7, marker=\"o\", markersize=3)\n",
    "        ax.fill_between(detection_points, zero_line, prob_points, where=(zero_line > prob_points), color='blue', alpha=0.3, interpolate=True)\n",
    "        ax.fill_between(detection_points, zero_line, prob_points, where=(zero_line <= prob_points), color='red', alpha=0.3, interpolate=True)\n",
    "        \n",
    "        ax.tick_params(axis='both', which='major', labelsize=24)\n",
    "        ax.set_title(f\"Detection on segments with {SLICE_SIZE} samples overlapping at {SLICE_OVERLAP} samples\", fontsize=24)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(title, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_path = \"real.wav\"\n",
    "fake_path = \"fake.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_controller = 0\n",
    "min_slice_size = 3200\n",
    "max_slice_size = 6400\n",
    "SLICE_SIZE_AND_OVERLAPS = [(2400, 1200), (4000, 2000), (6400, 3200), (8000, 4000)]\n",
    "probs_list = []\n",
    "\n",
    "for (SLICE_SIZE, SLICE_OVERLAP) in SLICE_SIZE_AND_OVERLAPS:\n",
    "    audio, parts, labels, probs = get_slice_probs(real_path, fake_path)\n",
    "    probs_list.append(probs)\n",
    "draw_plot(audio, parts, labels, probs_list, SLICE_SIZE_AND_OVERLAPS, \"3200-6400.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_controller = 0\n",
    "min_slice_size = 6400\n",
    "max_slice_size = 9600\n",
    "SLICE_SIZE_AND_OVERLAPS = [(2400, 1200), (4000, 2000), (6400, 3200), (8000, 4000)]\n",
    "probs_list = []\n",
    "\n",
    "for (SLICE_SIZE, SLICE_OVERLAP) in SLICE_SIZE_AND_OVERLAPS:\n",
    "    audio, parts, labels, probs = get_slice_probs(real_path, fake_path)\n",
    "    probs_list.append(probs)\n",
    "draw_plot(audio, parts, labels, probs_list, SLICE_SIZE_AND_OVERLAPS, \"6400-9600.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_controller = 0\n",
    "min_slice_size = 9600\n",
    "max_slice_size = 12800\n",
    "SLICE_SIZE_AND_OVERLAPS = [(2400, 1200), (4000, 2000), (6400, 3200), (8000, 4000)]\n",
    "probs_list = []\n",
    "\n",
    "for (SLICE_SIZE, SLICE_OVERLAP) in SLICE_SIZE_AND_OVERLAPS:\n",
    "    audio, parts, labels, probs = get_slice_probs(real_path, fake_path)\n",
    "    probs_list.append(probs)\n",
    "draw_plot(audio, parts, labels, probs_list, SLICE_SIZE_AND_OVERLAPS, \"9600-12800.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_controller = 0\n",
    "min_slice_size = 12800\n",
    "max_slice_size = 16000\n",
    "SLICE_SIZE_AND_OVERLAPS = [(2400, 1200), (4000, 2000), (6400, 3200), (8000, 4000)]\n",
    "probs_list = []\n",
    "\n",
    "for (SLICE_SIZE, SLICE_OVERLAP) in SLICE_SIZE_AND_OVERLAPS:\n",
    "    audio, parts, labels, probs = get_slice_probs(real_path, fake_path)\n",
    "    probs_list.append(probs)\n",
    "draw_plot(audio, parts, labels, probs_list, SLICE_SIZE_AND_OVERLAPS, \"12800-16000.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "490-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
